{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jiangxinfa/program/paddlepaddle_proj/QA_baidu_2/src\n",
      "/home/jiangxinfa/program/paddlepaddle_proj/QA_baidu_2/src\n",
      "/home/jiangxinfa/program/paddlepaddle_proj/QA_baidu_2/src\n",
      "/home/jiangxinfa/program/paddlepaddle_proj/QA_baidu_2/src\n",
      "/home/jiangxinfa/program/paddlepaddle_proj/QA_baidu_2/src\n",
      "/home/jiangxinfa/program/paddlepaddle_proj/QA_baidu_2/src\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import src.data as data\n",
    "from src.config import USE_CUDA, DEVICE\n",
    "from src.data import Vocab\n",
    "from src.model import Model\n",
    "from src.batcher import Example\n",
    "from src.batcher import Batch\n",
    "from src.batcher import get_input_from_batch\n",
    "import src.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch_by_article(article, vocab):\n",
    "    words = jieba.cut(article)\n",
    "    art_str = \" \".join(words)\n",
    "    example = Example(art_str, [\"\",], vocab)\n",
    "    ex_list = [example for _ in range(config.beam_size)]\n",
    "    batch  = Batch(ex_list, vocab, config.beam_size)\n",
    "    return batch\n",
    "\n",
    "\"\"\"\n",
    "decode阶段使用 beam search 算法\n",
    "\"\"\"\n",
    "class Beam(object):\n",
    "    def __init__(self, tokens, log_probs, state, context, coverage):\n",
    "        self.tokens = tokens\n",
    "        self.log_probs = log_probs\n",
    "        self.state = state\n",
    "        self.context = context\n",
    "        self.coverage = coverage\n",
    "\n",
    "    def extend(self, token, log_prob, state, context, coverage):\n",
    "        return Beam(tokens = self.tokens + [token],\n",
    "                            log_probs = self.log_probs + [log_prob],\n",
    "                            state = state,\n",
    "                            context = context,\n",
    "                            coverage = coverage)\n",
    "\n",
    "    @property\n",
    "    def latest_token(self):\n",
    "        return self.tokens[-1]\n",
    "\n",
    "    @property\n",
    "    def avg_log_prob(self):\n",
    "        return sum(self.log_probs) / len(self.tokens)\n",
    "\n",
    "\n",
    "class BeamSearch(object):\n",
    "    def __init__(self, model_file_path, vocab):\n",
    "        self.vocab = vocab\n",
    "        # 加载模型\n",
    "        self.model = Model(model_file_path, is_eval=True)\n",
    "\n",
    "    def sort_beams(self, beams):\n",
    "        return sorted(beams, key=lambda h: h.avg_log_prob, reverse=True)\n",
    "\n",
    "    def decode(self, batch):\n",
    "        best_summary = self.beam_search(batch)\n",
    "\n",
    "        # Extract the output ids from the hypothesis and convert back to words\n",
    "        output_ids = [int(t) for t in best_summary.tokens[1:]]\n",
    "        decoded_words = data.outputids2words(output_ids, self.vocab,\n",
    "                                                (batch.art_oovs[0] if config.pointer_gen else None))\n",
    "        # Remove the [STOP] token from decoded_words, if necessary\n",
    "        try:\n",
    "            fst_stop_idx = decoded_words.index(data.STOP_DECODING)\n",
    "            decoded_words = decoded_words[:fst_stop_idx]\n",
    "        except ValueError:\n",
    "            decoded_words = decoded_words\n",
    "        return \"\".join(decoded_words)\n",
    "\n",
    "\n",
    "    def beam_search(self, batch):\n",
    "        # batch should have only one example\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, c_t_0, coverage_t_0 = \\\n",
    "            get_input_from_batch(batch)\n",
    "\n",
    "        encoder_outputs, encoder_feature, encoder_hidden = self.model.encoder(enc_batch, enc_lens)\n",
    "        s_t_0 = self.model.reduce_state(encoder_hidden)\n",
    "\n",
    "        dec_h, dec_c = s_t_0 # 1 x 2*hidden_size\n",
    "        dec_h = dec_h.squeeze()\n",
    "        dec_c = dec_c.squeeze()\n",
    "\n",
    "        # decoder batch preparation, it has beam_size example initially everything is repeated\n",
    "        beams = [Beam(tokens=[self.vocab.word2id(data.START_DECODING)],\n",
    "                      log_probs=[0.0],\n",
    "                      state=(dec_h[0], dec_c[0]),\n",
    "                      context = c_t_0[0],\n",
    "                      coverage=(coverage_t_0[0] if config.is_coverage else None))\n",
    "                 for _ in range(config.beam_size)]\n",
    "        results = []\n",
    "        steps = 0\n",
    "        while steps < config.max_dec_steps and len(results) < config.beam_size:\n",
    "            latest_tokens = [h.latest_token for h in beams]\n",
    "            latest_tokens = [t if t < self.vocab.size() else self.vocab.word2id(data.UNKNOWN_TOKEN) \\\n",
    "                             for t in latest_tokens]\n",
    "            y_t_1 = Variable(torch.LongTensor(latest_tokens))\n",
    "            if USE_CUDA:\n",
    "                y_t_1 = y_t_1.to(DEVICE)\n",
    "            all_state_h =[]\n",
    "            all_state_c = []\n",
    "\n",
    "            all_context = []\n",
    "\n",
    "            for h in beams:\n",
    "                state_h, state_c = h.state\n",
    "                all_state_h.append(state_h)\n",
    "                all_state_c.append(state_c)\n",
    "\n",
    "                all_context.append(h.context)\n",
    "\n",
    "            s_t_1 = (torch.stack(all_state_h, 0).unsqueeze(0), torch.stack(all_state_c, 0).unsqueeze(0))\n",
    "            c_t_1 = torch.stack(all_context, 0)\n",
    "\n",
    "            coverage_t_1 = None\n",
    "            if config.is_coverage:\n",
    "                all_coverage = []\n",
    "                for h in beams:\n",
    "                    all_coverage.append(h.coverage)\n",
    "                coverage_t_1 = torch.stack(all_coverage, 0)\n",
    "\n",
    "            final_dist, s_t, c_t, attn_dist, p_gen, coverage_t = self.model.decoder(y_t_1, s_t_1,\n",
    "                                                        encoder_outputs, encoder_feature, enc_padding_mask, c_t_1,\n",
    "                                                        extra_zeros, enc_batch_extend_vocab, coverage_t_1, steps)\n",
    "            log_probs = torch.log(final_dist)\n",
    "            topk_log_probs, topk_ids = torch.topk(log_probs, config.beam_size * 2)\n",
    "\n",
    "            dec_h, dec_c = s_t\n",
    "            dec_h = dec_h.squeeze()\n",
    "            dec_c = dec_c.squeeze()\n",
    "\n",
    "            all_beams = []\n",
    "            num_orig_beams = 1 if steps == 0 else len(beams)\n",
    "            for i in range(num_orig_beams):\n",
    "                h = beams[i]\n",
    "                state_i = (dec_h[i], dec_c[i])\n",
    "                context_i = c_t[i]\n",
    "                coverage_i = (coverage_t[i] if config.is_coverage else None)\n",
    "\n",
    "                for j in range(config.beam_size * 2):  # for each of the top 2*beam_size hyps:\n",
    "                    new_beam = h.extend(token=topk_ids[i, j].item(),\n",
    "                                   log_prob=topk_log_probs[i, j].item(),\n",
    "                                   state=state_i,\n",
    "                                   context=context_i,\n",
    "                                   coverage=coverage_i)\n",
    "                    all_beams.append(new_beam)\n",
    "\n",
    "            beams = []\n",
    "            for h in self.sort_beams(all_beams):\n",
    "                if h.latest_token == self.vocab.word2id(data.STOP_DECODING):\n",
    "                    if steps >= config.min_dec_steps:\n",
    "                        results.append(h)\n",
    "                else:\n",
    "                    beams.append(h)\n",
    "                if len(beams) == config.beam_size or len(results) == config.beam_size:\n",
    "                    break\n",
    "\n",
    "            steps += 1\n",
    "        if len(results) == 0:\n",
    "            results = beams\n",
    "    \n",
    "        beams_sorted = self.sort_beams(results)\n",
    "        return beams_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path='data/AutoMaster_TestSet.csv'\n",
    "model_path='logs/model/model_0'\n",
    "save_path='result/result.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试文件路径\n",
    "test_df=pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df['merged'] = test_df[['Question', 'Dialogue']].apply(lambda x: ' '.join(x),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value=test_df['merged'].values\n",
    "test_qid=test_df['QID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 20000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_value),len(test_qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: 从边\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "beam_processor = BeamSearch(model_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_summary=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.635 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(test_value):\n",
    "    batch_i = build_batch_by_article(v, vocab)\n",
    "    out=beam_processor.decode(batch_i)\n",
    "    out_summary.append(out)\n",
    "    if i%100==0:\n",
    "        print(i/20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 20000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_summary),len(test_qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df['QID']=test_qid\n",
    "out_df['Prediction']=out_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv(save_path,index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch13_py37 Env",
   "language": "python",
   "name": "pytorch13_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
